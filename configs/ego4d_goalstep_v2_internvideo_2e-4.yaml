dataset_name: ego4d
track: goal_step
devices: cuda:0
train_split: ['training']
val_split: ['validation']
model_name: DynamicPointTransformer
dataset: {
  json_file: ego4d_data/goalstep_data/ego4d_goal_step_val_v2.jsonl,
  train_jsonl_file: ego4d_data/goalstep_data/ego4d_goal_step_train_v2.jsonl,
  val_jsonl_file: ego4d_data/goalstep_data/ego4d_goal_step_val_v2.jsonl,
  video_feat_dir: /root/autodl-tmp/data/ego4d/goalstep/internvideo_lmdb,
  text_feat_dir: /root/autodl-tmp/data/ego4d/goalstep/clip_query_lmdb,
  val_text_feat_dir: /root/autodl-tmp/data/ego4d/goalstep/clip_query_lmdb,
  num_classes: 1,
  input_vid_dim: 2304,
  input_txt_dim: 512,
  feat_stride: 16.0,
  num_frames: 16.0,
  default_fps: 30,

  max_seq_len: 33280,
}
model: {
  fpn_type: identity,
  max_buffer_len_factor: 4.0,
  n_mha_win_size: 9,
  backbone_arch: [2, 4, 4, 0, 6],
  # shrink the model for reduced input feature channels
  n_head: 4,
  embd_dim: 384,
  fpn_dim: 384,
  head_dim: 384,
  use_abs_pe: True,
  regression_range: [[0, 4], [2, 8], [4, 16], [8, 32], [16, 64], [32, 128],[64, 10000]],
}
opt: {
  learning_rate: 0.0002,
  backbone_lr_weight: 1,
  epochs: 6,
  warmup_epochs: 4,
  weight_decay: 0.05,
}
loader: {
  batch_size: 2,
  num_workers: 2,
}
train_cfg: {
  init_loss_norm: 200,
  clip_grad_l2norm: 1.0,
  cls_prior_prob: 0.01,
  center_sample: radius,
  center_sample_radius: 1.5,
  label_smoothing: 0.1,
  droppath: 0.1,
  loss_weight: 1.0,
  debug: False,
}
test_cfg: {
  voting_thresh: 0.9,
  pre_nms_topk: 2000,
  # max of 50 predictions per video
  max_seg_num: 5,
  min_score: 0.001,
  nms_sigma : 0.75,
  duration_thresh: 0.001,
  test_num: 2,
  test_start_epoch: 4
}
output_folder: /root/autodl-tmp/model/GroundNLQ/goalstep/
